"""
DAG d'exemple : Import CSV employees depuis GCS vers BigQuery
Compatible avec le tutoriel 2 - Pipeline Airflow
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.google.cloud.sensors.gcs import GCSObjectExistenceSensor
from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyTableOperator, BigQueryInsertJobOperator
from airflow.operators.python import PythonOperator
from airflow.operators.email import EmailOperator
from airflow.utils.dates import days_ago
import logging

# Configuration par d√©faut
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,  # D√©sactiv√© pour l'environnement local
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

# Param√®tres du pipeline (√† adapter selon votre environnement)
PROJECT_ID = 'lake-471013'
DATASET_ID = 'lakehouse_employee_data'
TABLE_ID = 'employees'
GCS_BUCKET = 'lakehouse-bucket-20250903'
GCS_OBJECT = 'employees_5mb.csv'

# D√©finition du DAG
dag = DAG(
    'employees_csv_ingestion_local',
    default_args=default_args,
    description='Pipeline ingestion CSV employees vers BigQuery (Local K8s)',
    schedule_interval=None,  # Ex√©cution manuelle pour les tests locaux
    catchup=False,
    max_active_runs=1,
    tags=['bigquery', 'gcs', 'employees', 'ingestion', 'local']
)

def validate_csv_file(**context):
    """Valide la structure et le contenu du fichier CSV"""
    # Version simplifi√©e pour l'environnement local
    logging.info("üîç Validation du fichier CSV...")
    
    # Simulation de validation (en local, on suppose que le fichier est valide)
    expected_columns = ['id', 'nom', 'prenom', 'email', 'age', 'ville', 
                       'code_postal', 'telephone', 'salaire', 'departement',
                       'date_embauche', 'statut', 'score', 'latitude', 
                       'longitude', 'commentaire', 'reference', 'niveau', 
                       'categorie', 'timestamp']
    
    logging.info(f"‚úÖ Structure CSV attendue: {len(expected_columns)} colonnes")
    logging.info(f"   Colonnes: {', '.join(expected_columns)}")
    logging.info("‚úÖ Fichier CSV valid√© (simulation pour environnement local)")
    
    return True

def validate_loaded_data(**context):
    """Valide les donn√©es charg√©es en BigQuery"""
    logging.info("üîç Validation des donn√©es charg√©es...")
    
    # Simulation de validation (en local)
    # En production, cette fonction ferait des requ√™tes BigQuery r√©elles
    
    logging.info("‚úÖ Validation des donn√©es simul√©e:")
    logging.info("   - Donn√©es charg√©es avec succ√®s")
    logging.info("   - Pas de doublons d√©tect√©s")
    logging.info("   - Format email valid√©")
    logging.info("   - √Çges coh√©rents")
    
    return True

def log_pipeline_metrics(**context):
    """Log des m√©triques du pipeline pour monitoring"""
    logging.info("üìä M√©triques du pipeline:")
    logging.info(f"   - DAG: {context['dag'].dag_id}")
    logging.info(f"   - Run ID: {context['run_id']}")
    logging.info(f"   - Date d'ex√©cution: {context['execution_date']}")
    logging.info("   - Statut: SUCCESS")
    
    return "metrics_logged"

# TASK 1: Validation du fichier CSV
validate_csv = PythonOperator(
    task_id='validate_csv_structure',
    python_callable=validate_csv_file,
    dag=dag
)

# TASK 2: Simulation de l'ingestion BigQuery (sans vraie connexion GCP)
simulate_ingestion = PythonOperator(
    task_id='simulate_bigquery_ingestion',
    python_callable=lambda **context: logging.info("üöÄ Simulation de l'ingestion BigQuery termin√©e avec succ√®s"),
    dag=dag
)

# TASK 3: Validation des donn√©es charg√©es
validate_data = PythonOperator(
    task_id='validate_loaded_data',
    python_callable=validate_loaded_data,
    dag=dag
)

# TASK 4: Log des m√©triques
log_metrics = PythonOperator(
    task_id='log_pipeline_metrics',
    python_callable=log_pipeline_metrics,
    dag=dag
)

# TASK 5: Notification de succ√®s (sans email en local)
success_notification = PythonOperator(
    task_id='send_success_notification',
    python_callable=lambda **context: logging.info("‚úÖ Pipeline termin√© avec succ√®s! üéâ"),
    dag=dag
)

# D√©finition des d√©pendances entre tasks
validate_csv >> simulate_ingestion >> validate_data >> log_metrics >> success_notification

# Documentation du DAG
dag.doc_md = """
# Pipeline d'ingestion CSV Employees (Local Kubernetes)

Ce DAG orchestre l'ingestion des donn√©es employees depuis GCS vers BigQuery.
Version adapt√©e pour l'environnement Kubernetes local.

## Architecture:
1. ‚úÖ Validation de la structure du fichier CSV
2. üöÄ Simulation de l'ingestion BigQuery (remplace la vraie ingestion)
3. üîç Validation des donn√©es charg√©es (simulation)
4. üìä Collecte des m√©triques du pipeline
5. üéâ Notification de succ√®s

## Configuration:
- **Projet**: {PROJECT_ID}
- **Dataset**: {DATASET_ID}
- **Table**: {TABLE_ID}
- **Fichier source**: gs://{GCS_BUCKET}/{GCS_OBJECT}

## Utilisation:
Ce DAG est configur√© pour l'ex√©cution manuelle (`schedule_interval=None`).
Pour l'activer, allez dans l'interface Airflow et cliquez sur "Trigger DAG".

## Notes:
- Version simplifi√©e pour tests locaux
- Les vraies connexions GCP sont remplac√©es par des simulations
- Id√©al pour tester l'orchestration et le monitoring
""".format(
    PROJECT_ID=PROJECT_ID,
    DATASET_ID=DATASET_ID, 
    TABLE_ID=TABLE_ID,
    GCS_BUCKET=GCS_BUCKET,
    GCS_OBJECT=GCS_OBJECT
)