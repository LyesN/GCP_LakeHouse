# Tutoriel 1 : Import CSV GCS vers BigQuery - Bases et ingestion SQL

## R√©f√©rence des bonnes pratiques
üìö [Bonnes pratiques pour charger, transformer et exporter des donn√©es BigQuery](https://cloud.google.com/bigquery/docs/load-transform-export-intro?hl=fr)

## Contexte
- **Public cible** : Data Engineers, Data Analysts
- **Environnement** : GCP Console (environnement DEV)
- **Stack** : BigQuery
- **Objectif** : Cr√©er un pipeline de donn√©es automatis√©

## Param√®tres du projet
- **Projet GCP** : `LakeHouse`
- **BigQuery name** : `lake-471013`
- **Dataset BigQuery** : `lakehouse_employee_data`
- **Bucket GCS** : `lakehouse-bucket-20250903`
- **Fichier CSV** : `employees.csv`
- **Chemin complet** : `gs://lakehouse-bucket-20250903/employees.csv`

## Pr√©requis
- Permissions appropri√©es sur BigQuery et GCS

## Plan du tutoriel

1. **[√âtape 1](#√©tape-1--v√©rification-du-fichier-csv-dans-gcs)** : V√©rification du fichier CSV dans GCS
2. **[√âtape 2](#√©tape-2--d√©veloppement-sur-console-gcp)** : D√©veloppement sur Console GCP
   - Cr√©ation du dataset BigQuery
   - D√©finition de la table et du sch√©ma
   - D√©veloppement du flux d'ingestion en SQL

## Structure du fichier CSV exemple
Le fichier contient les colonnes suivantes avec s√©parateur `;` :
```
id;nom;prenom;email;age;ville;code_postal;telephone;salaire;departement;date_embauche;statut;score;latitude;longitude;commentaire;reference;niveau;categorie;timestamp
```

## √âtape 1 : Pr√©paration et v√©rification du fichier CSV dans GCS

### 1.1 Cr√©ation du bucket GCS

1. Acc√©dez √† la **GCP Console**
2. Naviguez vers **Cloud Storage**
3. Cliquez sur **Cr√©er un bucket**
4. Configurez le bucket :
   - **Nom du bucket** : `lakehouse-bucket-20250903`
   - **Type d'emplacement** : R√©gion
   - **R√©gion** : `us-east1` (us pour le Free Tier)
   - **Classe de stockage** : Par d√©faut (Standard)
   - **Contr√¥le d'acc√®s** : Par d√©faut
   - **Protection** : Pas de protection
5. Cliquez sur **Cr√©er**

### 1.2 Upload du fichier CSV

1. S√©lectionnez le bucket `lakehouse-bucket-20250903` cr√©√©
2. Cliquez sur **Importer des fichiers**
3. S√©lectionnez votre fichier `employees.csv` depuis votre syst√®me local
4. Attendez la fin de l'upload
5. V√©rifiez que le fichier appara√Æt dans la liste avec la taille attendue (~5MB)

### 1.3 V√©rification du fichier upload√©

1. **V√©rifier le fichier upload√©** :
   - Cliquez sur le fichier `employees.csv` pour voir ses d√©tails
   - Notez le chemin complet : `gs://lakehouse-bucket-20250903/employees.csv`
   - V√©rifiez que la taille est d'environ 5MB

## √âtape 2 : D√©veloppement sur Console GCP

### Use Case : d√©veloppement du flux d'ingestion dans la console BigQuery

### 2.1 Cr√©ation du dataset BigQuery

1. Dans la **GCP Console**, acc√©dez √† **BigQuery**
2. Dans l'explorateur, cliquez sur votre projet `lake-471013`
3. Cliquez sur **Cr√©er un dataset**
4. Configurez le dataset :
   - **ID du dataset** : `lakehouse_employee_data`
   - **Emplacement** : US (pour correspondre au bucket GCS)
   - **Expiration** : Par d√©faut ou selon votre politique d'entreprise
5. Cliquez sur **Cr√©er un dataset**

### 2.1.1 Test d'accessibilit√© du fichier GCS

Une fois le dataset cr√©√©, v√©rifiez l'accessibilit√© du fichier CSV :

1. **Test d'accessibilit√© depuis BigQuery** :
   - Dans l'explorateur BigQuery, cliquez sur le dataset `lakehouse_employee_data` 
   - Cliquez sur **+ Cr√©er une table**
   - **Source** : Google Cloud Storage
   - **Parcourir** : chercher le fichier dans le bucket `lakehouse-bucket-20250903`
   
2. **Validation des permissions** :
   - Si vous pouvez parcourir et s√©lectionner le fichier ‚Üí Permissions OK
   - Si le fichier n'appara√Æt pas ‚Üí Contactez votre administrateur GCP

2. **Quitter sans sauvegarder** :
   - "Annuler" tout en bas puis "Oui quitter"

### 2.2 Cr√©ation de la table avec sch√©ma d√©fini

1. **Ouvrir l'√©diteur SQL** :
   - Dans BigQuery, cliquez sur **+** (en haut du canvas)
   - Une nouvelle fen√™tre d'√©diteur SQL s'ouvre

2. **Saisir la requ√™te de cr√©ation de table** :

```sql
-- Cr√©ation de la table employees avec sch√©ma typ√©
CREATE TABLE `lake-471013.lakehouse_employee_data.employees` (
  id INT64 NOT NULL,
  nom STRING,
  prenom STRING,
  email STRING,
  age INT64,
  ville STRING,
  code_postal STRING,
  telephone STRING,
  salaire FLOAT64,
  departement STRING,
  date_embauche DATE,
  statut STRING,
  score FLOAT64,
  latitude FLOAT64,
  longitude FLOAT64,
  commentaire STRING,
  reference STRING,
  niveau STRING,
  categorie STRING,
  timestamp TIMESTAMP,
  -- M√©tadonn√©es d'ingestion
  ingestion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  source_file STRING
);
```

3. **Ex√©cuter la requ√™te** :
   - Cliquez sur **Ex√©cuter** (bouton bleu) ou utilisez Ctrl+Enter
   - V√©rifiez que la table appara√Æt dans l'explorateur sous `lakehouse_employee_data`
   - La table est maintenant cr√©√©e et pr√™te pour l'ingestion

### 2.3 D√©veloppement du flux d'ingestion en SQL

#### A. Requ√™te de chargement depuis GCS

```sql
-- Flux d'ingestion CSV vers BigQuery
-- Fichier source : employees
-- Table cible : employees.csv
-- Truncate avant bulk

TRUNCATE TABLE `lake-471013.lakehouse_employee_data.employees`;

LOAD DATA INTO `lake-471013.lakehouse_employee_data.employees`
(id INT64, nom STRING, prenom STRING, email STRING, age INT64, ville STRING, 
 code_postal STRING, telephone STRING, salaire FLOAT64, departement STRING, 
 date_embauche DATE, statut STRING, score FLOAT64, latitude FLOAT64, 
 longitude FLOAT64, commentaire STRING, reference STRING, niveau STRING, 
 categorie STRING, timestamp TIMESTAMP)
FROM FILES (
  format = 'CSV',
  field_delimiter = ';',
  skip_leading_rows = 1,
  uris = ['gs://lakehouse-bucket-20250903/employees.csv']
);
```

**Avantages du sch√©ma forc√©** :
- **√âvite les conflits** : BigQuery n'essaie pas de d√©tecter automatiquement le sch√©ma
- **Contr√¥le total** : Impose exactement les types de donn√©es souhait√©s
- **Robustesse** : √âvite les erreurs "Field has changed mode/type"
- **Performance** : Pas de phase de d√©tection automatique

## Prochaines √©tapes

Ce tutoriel couvre les bases de l'ingestion CSV vers BigQuery. Pour continuer :

- **Tutoriel 2** : Impl√©mentation avec Airflow pour l'orchestration
- **Tutoriel 3** : Impl√©mentation avec Dataflow pour le processing √† grande √©chelle

## Bonnes pratiques

- Toujours forcer le sch√©ma pour √©viter les conflits
- Tester l'accessibilit√© des fichiers avant l'ingestion
- Utiliser des m√©tadonn√©es d'ingestion pour tra√ßabilit√©
- Valider la structure du fichier CSV avant traitement