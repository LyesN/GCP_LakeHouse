# Framework GCP Data Lakehouse

## Introduction

Ce framework d√©finit les standards et patterns d'impl√©mentation pour la construction de pipelines de donn√©es sur Google Cloud Platform. Il vise √† assurer la coh√©rence, la maintenabilit√© et la scalabilit√© des solutions data.

## Stack Technique Autoris√©

Les d√©veloppements du projet sont **strictement limit√©s** aux services GCP suivants :

| Service | R√¥le | Usage |
|---------|------|-------|
| **BigQuery** | Data Warehouse | Stockage et requ√™tage des donn√©es structur√©es |
| **Dataform** | ELT/Orchestration | Transformations SQL et gestion des d√©pendances |
| **Cloud Storage** | Data Lake | Stockage des fichiers raw (CSV, JSON, Parquet) |
| **Cloud Composer** | Orchestration | Workflows complexes et scheduling |
| **Cloud Function** | Microservices | Fonctions serverless pour √©v√©nements |
| **Dataproc** | Big Data Processing | Traitement Spark/Hadoop si n√©cessaire |
| **Cloud Run** | Services | Applications conteneuris√©es |

## Architecture en Couches

Le framework impose une architecture en **3 couches** :

```
RAW (Cloud Storage) ‚Üí STG (01_STG) ‚Üí ODS (02_ODS)
```

### Couche RAW
- **Stockage** : Cloud Storage buckets
- **Format** : Fichiers bruts (CSV, JSON, Parquet)
- **Naming** : `gs://{project}-bucket-{date}/`

### Couche STG (Staging - 01_STG)
- **Stockage** : Tables externes BigQuery
- **R√¥le** : Acc√®s direct aux fichiers raw sans ingestion
- **Schema** : `01_STG`
- **Convention** : `01_STG.{entity_name}`

### Couche ODS (Operational Data Store - 02_ODS)
- **Stockage** : Tables BigQuery mat√©rialis√©es
- **R√¥le** : Donn√©es nettoy√©es et transform√©es
- **Schema** : `02_ODS`
- **Convention** : `02_ODS.{entity_name}`
- **M√©tadonn√©es** : Ajout automatique de `ingestion_date` et `source_file`

## Patterns d'Impl√©mentation

### Pattern Ingestion CSV Simple

**Use Case** : Ingestion d'un fichier CSV depuis GCS vers BigQuery

**Flux** :
```
Fichier CSV (GCS) ‚Üí Table Externe (01_STG) ‚Üí Table Mat√©rialis√©e (02_ODS)
```

**Avantages** :
- ‚úÖ Traitement full BigQuery
- ‚úÖ Pas d'infrastructure processing suppl√©mentaire
- ‚úÖ Lineage et observabilit√© de bout en bout
- ‚úÖ Performance optimale pour donn√©es structur√©es

**Template des livrables** :
```
bigquery/00_ddl/create_external_table_stg_{entity}.sql
bigquery/00_ddl/create_table_ods_{entity}.sql
Dataform/02_ods/load_stg_to_ods_{entity}.sqlx
```

**Contraintes** :
- Format source : CSV avec d√©limiteur d√©fini
- Taille max recommand√©e : < 5GB par fichier
- Schema fixe et typ√©

### Pattern Ingestion avec Orchestration Airflow

**Use Case** : Pipeline complexe avec √©tapes conditionnelles

**Flux** :
```
GCS ‚Üí STG ‚Üí ODS (orchestr√© par Cloud Composer)
```

**Avantages** :
- ‚úÖ Gestion des erreurs et reprises
- ‚úÖ Workflows conditionnels
- ‚úÖ Monitoring avanc√©
- ‚úÖ Int√©gration avec syst√®mes externes

## Standards de Nommage

### Fichiers
- **DDL** : `create_{type}_{schema}_{entity}.sql`
- **Dataform** : `load_{source}_to_{target}_{entity}.sqlx`
- **Buckets** : `{project}-bucket-{yyyymmdd}`

### Tables
- **STG** : `01_STG.{entity_name}`
- **ODS** : `02_ODS.{entity_name}`

### Colonnes M√©tadonn√©es (Obligatoires en ODS)
```sql
ingestion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
source_file STRING
```

## Standards de D√©veloppement

### Templates de R√©f√©rence

Utiliser les fichiers de l'impl√©mentation **employees** comme templates de r√©f√©rence pour tous les nouveaux cas d'usage :

**Fichiers Templates** :
- `bigquery/00_ddl/create_external_table_stg_employees.sql` - Table externe STG
- `bigquery/00_ddl/create_table_ods_employees.sql` - Table ODS
- `Dataform/02_ods/load_stg_to_ods_employees.sqlx` - Transformation ELT

**R√®gle** : Dupliquer ces templates et adapter pour la nouvelle entit√© en respectant les conventions de nommage.

## Bonnes Pratiques

### Performance
- Utiliser le partitioning sur les dates en ODS
- Privil√©gier les tables externes pour STG
- Optimiser les requ√™tes SELECT avec WHERE

### Monitoring
- Ajouter syst√©matiquement les m√©tadonn√©es d'ingestion
- Utiliser les vues Dataform pour le lineage


## Processus d'Impl√©mentation

### 1. Analyse des Besoins
- Identifier le pattern applicable
- D√©finir l'entit√© et le schema
- Valider les contraintes techniques

### 2. D√©veloppement
- Suivre les templates du pattern choisi
- Respecter les conventions de nommage

### 3. Tests et Validation
- V√©rifier le lineage de bout en bout
- Valider les types et contraintes
- Tester la performance

### 4. D√©ploiement
- D√©ployer STG puis ODS
- V√©rifier les permissions
- Monitorer l'ex√©cution

## Ressources et R√©f√©rences

### Documentation Technique
- [Bonnes pratiques BigQuery](https://cloud.google.com/bigquery/docs/load-transform-export-intro?hl=fr)
- [Guide Dataform](https://cloud.google.com/dataform/docs)

### Documentation D√©taill√©e
- üìã **[Architecture Lakehouse Aliment√© avec Fichier](docs/architecture-lakehouse-alimente-fichier.md)** - Architecture visuelle et exemples concrets

### Tutoriels du Projet
- `tutos/tuto1_bases_ingestion_sql.md` - Bases de l'ingestion CSV
- `tutos/tuto3_pipeline_dataform.md` - Pipeline avec tables externes

### Templates de Code
- `bigquery/00_ddl/` - Scripts DDL de cr√©ation de tables
- `Dataform/02_ods/` - Transformations ELT


## √âvolutions du Framework

Ce framework est √©volutif. Les nouveaux patterns doivent :
1. Respecter l'architecture en couches
2. Utiliser uniquement la stack autoris√©e
3. Suivre les conventions de nommage
4. √ätre document√©s avec un tutoriel associ√©