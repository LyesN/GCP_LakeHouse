# Tutoriel : Importer un fichier CSV depuis Google Cloud Storage vers BigQuery

## R√©f√©rence des bonnes pratiques
üìö [Bonnes pratiques pour charger, transformer et exporter des donn√©es BigQuery](https://cloud.google.com/bigquery/docs/load-transform-export-intro?hl=fr)

## Contexte
- **Public cible** : Data Engineers, Data Analysts
- **Environnement** : GCP Console (environnement DEV)
- **Stack** : BigQuery
- **Objectif** : Cr√©er un pipeline de donn√©es automatis√©

## Param√®tres du projet
- **Projet GCP** : `LakeHouse`
- **BigQuery name** : `lake-471013`
- **Dataset BigQuery** : `lakehouse_employee_data`
- **Bucket GCS** : `lakehouse-bucket-20250903`
- **Fichier CSV** : `employees_5mb.csv`
- **Chemin complet** : `gs://lakehouse-bucket-20250903/employees_5mb.csv`

## Pr√©requis
- Permissions appropri√©es sur BigQuery et GCS

## Plan du tutoriel

1. **[√âtape 1](#√©tape-1--v√©rification-du-fichier-csv-dans-gcs)** : V√©rification du fichier CSV dans GCS
2. **[√âtape 2](#√©tape-2--d√©veloppement-sur-console-gcp)** : D√©veloppement sur Console GCP
   - Cr√©ation du dataset BigQuery
   - D√©finition de la table et du sch√©ma
   - D√©veloppement du flux d'ingestion en SQL
3. **[√âtape 3](#√©tape-3--test-et-validation-du-flux)** : Test et validation du flux d'ingestion
4. **[√âtape 4](#√©tape-4--int√©gration-airflow-comme-orchestrateur)** : Int√©gration Airflow comme orchestrateur
   - Architecture du trigger Airflow
   - Configuration des connexions GCP
   - Monitoring et alertes
5. **[√âtape 5](#√©tape-5--d√©ploiement-et-monitoring-de-production)** : D√©ploiement et monitoring de production
6. **[√âtape 6](#√©tape-6--optimisations-pour-lentreprise)** : Optimisations pour l'entreprise
   - Partitioning et clustering
   - Gestion des donn√©es sensibles
7. **[Bonnes pratiques](#bonnes-pratiques)** et **[D√©pannage](#d√©pannage)**

## Structure du fichier CSV exemple
Le fichier contient les colonnes suivantes avec s√©parateur `;` :
```
id;nom;prenom;email;age;ville;code_postal;telephone;salaire;departement;date_embauche;statut;score;latitude;longitude;commentaire;reference;niveau;categorie;timestamp
```

## √âtape 1 : Pr√©paration et v√©rification du fichier CSV dans GCS

### 1.1 Cr√©ation du bucket GCS

1. Acc√©dez √† la **GCP Console**
2. Naviguez vers **Cloud Storage**
3. Cliquez sur **Cr√©er un bucket**
4. Configurez le bucket :
   - **Nom du bucket** : `lakehouse-bucket-20250903`
   - **Type d'emplacement** : R√©gion
   - **R√©gion** : `us-east1` (us pour le Free Tier)
   - **Classe de stockage** : Par d√©faut (Standard)
   - **Contr√¥le d'acc√®s** : Par d√©faut
   - **Protection** : Pas de protection
5. Cliquez sur **Cr√©er**

### 1.2 Upload du fichier CSV

1. S√©lectionnez le bucket `lakehouse-bucket-20250903` cr√©√©
2. Cliquez sur **Importer des fichiers**
3. S√©lectionnez votre fichier `employees_5mb.csv` depuis votre syst√®me local
4. Attendez la fin de l'upload
5. V√©rifiez que le fichier appara√Æt dans la liste avec la taille attendue (~5MB)

### 1.3 V√©rification du fichier upload√©

1. **V√©rifier le fichier upload√©** :
   - Cliquez sur le fichier `employees_5mb.csv` pour voir ses d√©tails
   - Notez le chemin complet : `gs://lakehouse-bucket-20250903/employees_5mb.csv`
   - V√©rifiez que la taille est d'environ 5MB

## √âtape 2 : D√©veloppement sur Console GCP

### Use Case : d√©veloppement du flux d'ingestion dans la console BigQuery

### 2.1 Cr√©ation du dataset BigQuery

1. Dans la **GCP Console**, acc√©dez √† **BigQuery**
2. Dans l'explorateur, cliquez sur votre projet `lake-471013`
3. Cliquez sur **Cr√©er un dataset**
4. Configurez le dataset :
   - **ID du dataset** : `lakehouse_employee_data`
   - **Emplacement** : US (pour correspondre au bucket GCS)
   - **Expiration** : Par d√©faut ou selon votre politique d'entreprise
5. Cliquez sur **Cr√©er un dataset**

### 2.1.1 Test d'accessibilit√© du fichier GCS

Une fois le dataset cr√©√©, v√©rifiez l'accessibilit√© du fichier CSV :

1. **Test d'accessibilit√© depuis BigQuery** :
   - Dans l'explorateur BigQuery, cliquez sur le dataset `lakehouse_employee_data` 
   - Cliquez sur **+ Cr√©er une table**
   - **Source** : Google Cloud Storage
   - **Parcourir** : chercher le fichier dans le bucket `lakehouse-bucket-20250903`
   
2. **Validation des permissions** :
   - Si vous pouvez parcourir et s√©lectionner le fichier ‚Üí Permissions OK
   - Si le fichier n'appara√Æt pas ‚Üí Contactez votre administrateur GCP

2. **Quitter sans sauvegarder** :
   - "Annuler" tout en bas puis "Oui quitter"

### 2.2 Cr√©ation de la table avec sch√©ma d√©fini

1. **Ouvrir l'√©diteur SQL** :
   - Dans BigQuery, cliquez sur **+** (en haut du canvas)
   - Une nouvelle fen√™tre d'√©diteur SQL s'ouvre

2. **Saisir la requ√™te de cr√©ation de table** :

```sql
-- Cr√©ation de la table employees avec sch√©ma typ√©
CREATE TABLE `lake-471013.lakehouse_employee_data.employees` (
  id INT64 NOT NULL,
  nom STRING,
  prenom STRING,
  email STRING,
  age INT64,
  ville STRING,
  code_postal STRING,
  telephone STRING,
  salaire FLOAT64,
  departement STRING,
  date_embauche DATE,
  statut STRING,
  score FLOAT64,
  latitude FLOAT64,
  longitude FLOAT64,
  commentaire STRING,
  reference STRING,
  niveau STRING,
  categorie STRING,
  timestamp TIMESTAMP,
  -- M√©tadonn√©es d'ingestion
  ingestion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  source_file STRING
);
```

3. **Ex√©cuter la requ√™te** :
   - Cliquez sur **Ex√©cuter** (bouton bleu) ou utilisez Ctrl+Enter
   - V√©rifiez que la table appara√Æt dans l'explorateur sous `lakehouse_employee_data`
   - La table est maintenant cr√©√©e et pr√™te pour l'ingestion

### 2.3 D√©veloppement du flux d'ingestion en SQL

#### A. Requ√™te de chargement depuis GCS

```sql
-- Flux d'ingestion principal depuis GCS avec sch√©ma forc√©
LOAD DATA INTO `lake-471013.lakehouse_employee_data.employees`
(id INT64, nom STRING, prenom STRING, email STRING, age INT64, ville STRING, 
 code_postal STRING, telephone STRING, salaire FLOAT64, departement STRING, 
 date_embauche DATE, statut STRING, score FLOAT64, latitude FLOAT64, 
 longitude FLOAT64, commentaire STRING, reference STRING, niveau STRING, 
 categorie STRING, timestamp TIMESTAMP)
FROM FILES (
  format = 'CSV',
  field_delimiter = ';',
  skip_leading_rows = 1,
  uris = ['gs://lakehouse-bucket-20250903/employees_5mb.csv']
);
```

**Avantages du sch√©ma forc√©** :
- **√âvite les conflits** : BigQuery n'essaie pas de d√©tecter automatiquement le sch√©ma
- **Contr√¥le total** : Impose exactement les types de donn√©es souhait√©s
- **Robustesse** : √âvite les erreurs "Field has changed mode/type"
- **Performance** : Pas de phase de d√©tection automatique

## √âtape 3 : Impl√©mentation et orchestration du pipeline avec Dataform

### 3.1 Introduction √† Dataform sur la Console GCP

**Dataform** est l'outil Google Cloud natif pour l'orchestration et la transformation des donn√©es dans BigQuery. Accessible directement depuis la Console GCP, il permet de :

- **Gestion de version** : Code SQL versionn√© et d√©ploy√© comme du code
- **Orchestration native** : D√©clenchement automatique des workflows
- **Tests int√©gr√©s** : Validation automatique de la qualit√© des donn√©es  
- **Documentation** : Documentation automatique des transformations
- **D√©pendances** : Gestion automatique des d√©pendances entre tables

### 3.2 Cr√©ation du projet Dataform dans la Console GCP

#### 3.2.1 Acc√®s √† Dataform

1. **Ouvrir la Console GCP** :
   - Connectez-vous √† [console.cloud.google.com](https://console.cloud.google.com)
   - S√©lectionnez votre projet `lake-471013`

2. **Naviguer vers Dataform** :
   - Dans le menu principal (‚ò∞), recherchez "Dataform"
   - Cliquez sur **Dataform** dans la section "Analytics"
   - Si c'est la premi√®re utilisation, activez l'API Dataform

#### 3.2.2 Cr√©ation du repository Dataform

1. **Cr√©er un nouveau repository** :
   - Cliquez sur **Cr√©er un repository**
   - **Nom du repository** : `lakehouse-employees-pipeline`
   - **R√©gion** : `us-east1` (pour correspondre √† BigQuery et GCS)
   - **Service Account** : Utilisez le service account par d√©faut ou cr√©ez-en un d√©di√©
   - Cliquez sur **Cr√©er**

2. **Configuration des permissions** :
   - V√©rifiez que le service account a les r√¥les :
     - `BigQuery Data Editor`
     - `BigQuery Job User` 
     - `Storage Object Viewer` (pour acc√©der aux fichiers GCS)

#### 3.2.3 Initialisation du workspace

1. **Cr√©er un workspace de d√©veloppement** :
   - Une fois le repository cr√©√©, cliquez sur **Cr√©er un workspace de d√©veloppement**
   - **Nom** : `dev-workspace`
   - Cliquez sur **Cr√©er un workspace**

2. **Acc√©der √† l'√©diteur** :
   - Cliquez sur le workspace cr√©√© pour ouvrir l'√©diteur Dataform int√©gr√©
   - Vous acc√©dez maintenant √† l'IDE Dataform dans le navigateur

### 3.3 Configuration du projet Dataform

#### 3.3.1 Fichier dataform.json

Dans l'√©diteur Dataform, cr√©ez/modifiez le fichier `dataform.json` :

1. **Cliquer sur le fichier `dataform.json`** dans l'explorateur de fichiers √† gauche
2. **Remplacer le contenu** par :

```json
{
  "defaultProject": "lake-471013",
  "defaultDataset": "lakehouse_employee_data",
  "defaultLocation": "US",
  "assertionSchema": "dataform_assertions",
  "warehouse": "bigquery",
  "defaultTags": ["lakehouse", "employees"],
  "vars": {
    "environment": "dev",
    "gcs_bucket": "lakehouse-bucket-20250903",
    "source_file": "employees_5mb.csv",
    "source_path": "gs://lakehouse-bucket-20250903/employees_5mb.csv"
  }
}
```

#### 3.3.2 Cr√©ation des datasets requis

Avant de continuer, cr√©er les datasets BigQuery n√©cessaires :

1. **Ouvrir BigQuery** dans un nouvel onglet
2. **Cr√©er les datasets** suivants :
   - `lakehouse_employee_data_raw` (pour les donn√©es brutes)
   - `lakehouse_employee_data_staging` (pour les donn√©es nettoy√©es)
   - `lakehouse_employee_data` (pour les tables finales - d√©j√† cr√©√©)
   - `dataform_assertions` (pour les tests qualit√©)

### 3.4 Cr√©ation des transformations Dataform

#### 3.4.1 Table source - Ingestion depuis GCS

1. **Cr√©er le dossier sources** :
   - Clic droit sur `definitions` ‚Üí **Nouveau dossier** ‚Üí `sources`

2. **Cr√©er le fichier employees_raw.sqlx** :
   - Clic droit sur `sources` ‚Üí **Nouveau fichier** ‚Üí `employees_raw.sqlx`
   - **Contenu du fichier** :

```sql
config {
  type: "operations",
  name: "load_employees_raw",
  description: "Chargement des donn√©es employ√©s depuis GCS vers BigQuery",
  tags: ["source", "raw", "employees"]
}

-- Cr√©er la table avec sch√©ma d√©fini
CREATE OR REPLACE TABLE `${dataform.projectConfig.defaultProject}.lakehouse_employee_data_raw.employees_raw` (
  id INT64 NOT NULL,
  nom STRING,
  prenom STRING,
  email STRING,
  age INT64,
  ville STRING,
  code_postal STRING,
  telephone STRING,
  salaire FLOAT64,
  departement STRING,
  date_embauche DATE,
  statut STRING,
  score FLOAT64,
  latitude FLOAT64,
  longitude FLOAT64,
  commentaire STRING,
  reference STRING,
  niveau STRING,
  categorie STRING,
  timestamp TIMESTAMP,
  -- M√©tadonn√©es d'ingestion
  ingestion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  source_file STRING DEFAULT '${dataform.projectConfig.vars.source_path}'
);

-- Chargement depuis GCS avec sch√©ma forc√©
LOAD DATA INTO `${dataform.projectConfig.defaultProject}.lakehouse_employee_data_raw.employees_raw`
(id INT64, nom STRING, prenom STRING, email STRING, age INT64, ville STRING,
 code_postal STRING, telephone STRING, salaire FLOAT64, departement STRING,
 date_embauche DATE, statut STRING, score FLOAT64, latitude FLOAT64,
 longitude FLOAT64, commentaire STRING, reference STRING, niveau STRING,
 categorie STRING, timestamp TIMESTAMP)
FROM FILES (
  format = 'CSV',
  field_delimiter = ';',
  skip_leading_rows = 1,
  uris = ['${dataform.projectConfig.vars.source_path}']
);
```

#### 3.4.2 Table de staging - Nettoyage des donn√©es

1. **Cr√©er le dossier staging** :
   - Clic droit sur `definitions` ‚Üí **Nouveau dossier** ‚Üí `staging`

2. **Cr√©er le fichier employees_clean.sqlx** :
   - Clic droit sur `staging` ‚Üí **Nouveau fichier** ‚Üí `employees_clean.sqlx`
   - **Contenu du fichier** :

```sql
config {
  type: "table",
  schema: "lakehouse_employee_data_staging",
  name: "employees_clean",
  description: "Donn√©es employ√©s nettoy√©es et valid√©es",
  tags: ["staging", "clean", "employees"],
  dependencies: ["load_employees_raw"]
}

SELECT 
  -- Identifiants
  id,
  
  -- Noms normalis√©s
  TRIM(UPPER(nom)) as nom,
  TRIM(INITCAP(prenom)) as prenom,
  TRIM(LOWER(email)) as email,
  
  -- Validation des √¢ges
  CASE 
    WHEN age BETWEEN 18 AND 65 THEN age 
    ELSE NULL 
  END as age,
  
  -- Donn√©es g√©ographiques
  TRIM(INITCAP(ville)) as ville,
  TRIM(code_postal) as code_postal,
  REGEXP_REPLACE(telephone, r'[^\d+]', '') as telephone_clean,
  
  -- Informations professionnelles
  CASE 
    WHEN salaire > 0 THEN salaire 
    ELSE NULL 
  END as salaire,
  TRIM(UPPER(departement)) as departement,
  date_embauche,
  TRIM(UPPER(statut)) as statut,
  
  -- Scores et coordonn√©es
  CASE 
    WHEN score BETWEEN 0 AND 100 THEN score 
    ELSE NULL 
  END as score,
  CASE 
    WHEN latitude BETWEEN -90 AND 90 THEN latitude 
    ELSE NULL 
  END as latitude,
  CASE 
    WHEN longitude BETWEEN -180 AND 180 THEN longitude 
    ELSE NULL 
  END as longitude,
  
  -- M√©tadonn√©es
  commentaire,
  reference,
  niveau,
  categorie,
  timestamp,
  
  -- M√©tadonn√©es d'ingestion et transformation
  ingestion_date,
  source_file,
  CURRENT_TIMESTAMP() as transformation_date,
  
  -- Indicateurs qualit√©
  CASE 
    WHEN email LIKE '%@%.%' THEN TRUE 
    ELSE FALSE 
  END as email_valid,
  CASE 
    WHEN age IS NULL OR salaire IS NULL THEN TRUE 
    ELSE FALSE 
  END as has_missing_data

FROM `${dataform.projectConfig.defaultProject}.lakehouse_employee_data_raw.employees_raw`
WHERE id IS NOT NULL
  AND nom IS NOT NULL 
  AND prenom IS NOT NULL
```

#### 3.4.3 Tables m√©tier (Data Marts)

1. **Cr√©er le dossier marts** :
   - Clic droit sur `definitions` ‚Üí **Nouveau dossier** ‚Üí `marts`

2. **Cr√©er dim_employees.sqlx** :
   - Clic droit sur `marts` ‚Üí **Nouveau fichier** ‚Üí `dim_employees.sqlx`

```sql
config {
  type: "table",
  schema: "lakehouse_employee_data",
  name: "dim_employees",
  description: "Dimension des employ√©s pour les analyses m√©tier",
  tags: ["marts", "dimension", "employees"],
  dependencies: ["employees_clean"]
}

SELECT 
  -- Cl√© primaire
  id as employee_id,
  
  -- Informations personnelles
  CONCAT(prenom, ' ', nom) as full_name,
  nom as last_name,
  prenom as first_name,
  email,
  age,
  
  -- Informations g√©ographiques
  ville as city,
  code_postal as postal_code,
  telephone_clean as phone,
  latitude,
  longitude,
  
  -- Informations professionnelles
  departement as department,
  statut as status,
  niveau as level,
  categorie as category,
  date_embauche as hire_date,
  
  -- Calculs d√©riv√©s
  DATE_DIFF(CURRENT_DATE(), date_embauche, YEAR) as years_of_service,
  CASE 
    WHEN DATE_DIFF(CURRENT_DATE(), date_embauche, YEAR) >= 5 THEN 'Senior'
    WHEN DATE_DIFF(CURRENT_DATE(), date_embauche, YEAR) >= 2 THEN 'Confirm√©'
    ELSE 'Junior'
  END as seniority_level,
  
  -- Indicateurs qualit√©
  email_valid,
  has_missing_data,
  
  -- M√©tadonn√©es
  transformation_date,
  CURRENT_TIMESTAMP() as dim_created_at

FROM ${ref("employees_clean")}
WHERE departement IS NOT NULL
  AND statut IN ('ACTIF', 'ACTIVE', 'EN_POSTE')
```

3. **Cr√©er fact_salaries.sqlx** :

```sql
config {
  type: "table",
  schema: "lakehouse_employee_data",
  name: "fact_salaries",
  description: "Table de faits pour l'analyse des salaires",
  tags: ["marts", "fact", "salaries"],
  dependencies: ["employees_clean"]
}

SELECT 
  -- Cl√©s
  id as employee_id,
  
  -- Mesures
  salaire as salary_amount,
  score as performance_score,
  
  -- Dimensions
  departement as department,
  niveau as level,
  age as employee_age,
  DATE_DIFF(CURRENT_DATE(), date_embauche, YEAR) as years_of_service,
  
  -- Calculs analytiques
  PERCENT_RANK() OVER (
    PARTITION BY departement 
    ORDER BY salaire
  ) * 100 as salary_percentile_dept,
  
  AVG(salaire) OVER (PARTITION BY departement) as avg_salary_dept,
  
  -- Classification salariale
  CASE 
    WHEN salaire >= 80000 THEN 'High'
    WHEN salaire >= 50000 THEN 'Medium' 
    ELSE 'Low'
  END as salary_band,
  
  -- M√©tadonn√©es
  transformation_date,
  CURRENT_TIMESTAMP() as fact_created_at

FROM ${ref("employees_clean")}
WHERE salaire IS NOT NULL 
  AND salaire > 0
  AND departement IS NOT NULL
```

### 3.5 Tests de qualit√© des donn√©es

1. **Cr√©er le fichier tests/data_quality.sqlx** :
   - Cr√©er le dossier `tests` dans `definitions`
   - Cr√©er le fichier `data_quality.sqlx`

```sql
config {
  type: "assertion",
  name: "employees_data_quality_tests",
  description: "Tests de qualit√© sur les donn√©es employ√©s",
  dependencies: ["employees_clean"]
}

-- Test: Pas de doublons sur les IDs
SELECT 
  COUNT(*) = 0 as test_passed
FROM (
  SELECT id, COUNT(*) as cnt
  FROM ${ref("employees_clean")}
  GROUP BY id
  HAVING COUNT(*) > 1
)
```

### 3.6 Compilation et ex√©cution du pipeline

#### 3.6.1 Compilation du projet

1. **Compiler le projet** :
   - Cliquez sur **Compiler** en haut de l'√©diteur
   - V√©rifiez qu'il n'y a pas d'erreurs de syntaxe
   - Les d√©pendances sont automatiquement calcul√©es

#### 3.6.2 Ex√©cution du workflow

1. **Ex√©cuter le workflow complet** :
   - Cliquez sur **Ex√©cuter** ‚Üí **Ex√©cuter toutes les actions**
   - S√©lectionnez les actions √† ex√©cuter dans l'ordre :
     1. `load_employees_raw`
     2. `employees_clean` 
     3. `employees_data_quality_tests`
     4. `dim_employees`
     5. `fact_salaries`

2. **Suivre l'ex√©cution** :
   - L'interface affiche le statut en temps r√©el
   - Les logs d√©taill√©s sont disponibles pour chaque action
   - Les erreurs sont mises en √©vidence avec des d√©tails

#### 3.6.3 V√©rification des r√©sultats

1. **V√©rifier dans BigQuery** :
   - Ouvrir BigQuery dans un nouvel onglet
   - V√©rifier que toutes les tables ont √©t√© cr√©√©es
   - Contr√¥ler le nombre de lignes dans chaque table

```sql
-- V√©rification rapide
SELECT 'employees_raw' as table_name, COUNT(*) as row_count
FROM `lake-471013.lakehouse_employee_data_raw.employees_raw`
UNION ALL
SELECT 'employees_clean', COUNT(*)
FROM `lake-471013.lakehouse_employee_data_staging.employees_clean`
UNION ALL  
SELECT 'dim_employees', COUNT(*)
FROM `lake-471013.lakehouse_employee_data.dim_employees`
UNION ALL
SELECT 'fact_salaries', COUNT(*)
FROM `lake-471013.lakehouse_employee_data.fact_salaries`
```

### 3.7 Orchestration avec des workflows programm√©s

#### 3.7.1 Cr√©ation d'un workflow release

1. **Cr√©er une release** :
   - Dans Dataform, aller dans l'onglet **Releases**
   - Cliquez sur **Cr√©er une release**
   - **Nom** : `v1.0-employees-pipeline`
   - **Configuration Git** : Branch `main`

2. **Configuration du workflow d'ex√©cution** :
   - Aller dans **Workflow Configurations**
   - Cliquer sur **Cr√©er une configuration de workflow**
   - **Nom** : `daily-employees-ingestion`
   - **Release** : S√©lectionner `v1.0-employees-pipeline`
   - **Fr√©quence** : `0 2 * * *` (tous les jours √† 2h du matin)

### 3.8 Monitoring et observabilit√©

#### 3.8.1 Dashboard d'ex√©cution Dataform

Dans l'interface Dataform :

1. **Onglet "Workflow Invocations"** : 
   - Historique de toutes les ex√©cutions
   - Dur√©es et statuts des actions
   - Logs d√©taill√©s par action

2. **Graphique de d√©pendances** :
   - Visualisation du DAG des transformations
   - Identification des goulots d'√©tranglement

#### 3.8.2 Int√©gration avec Cloud Monitoring

```sql
-- Requ√™te de monitoring dans BigQuery
SELECT 
  workflow_invocation_id,
  action_name,
  target,
  status,
  start_time,
  end_time,
  TIMESTAMP_DIFF(end_time, start_time, SECOND) as duration_seconds
FROM `lake-471013.dataform_monitoring.workflow_invocation_actions`
WHERE DATE(start_time) >= CURRENT_DATE() - 7
ORDER BY start_time DESC
```

### 3.9 Bonnes pratiques Dataform sur Console GCP

#### **Organisation du projet** :
- **Structure claire** : sources ‚Üí staging ‚Üí marts
- **Nommage coh√©rent** : Pr√©fixes par couche (raw_, clean_, dim_, fact_)
- **Documentation** : Description d√©taill√©e dans les configs
- **Tags** : √âtiquetage pour filtrer et organiser

#### **Gestion des environnements** :
- **Variables d'environnement** : Utiliser `dataform.json` vars
- **Workspaces s√©par√©s** : dev, staging, prod
- **Releases versionn√©es** : Git-based deployments

#### **Tests et qualit√©** :
- **Assertions obligatoires** : Tests sur chaque transformation
- **Validation des donn√©es** : Contr√¥les m√©tier int√©gr√©s
- **Monitoring** : Surveillance des m√©triques de qualit√©

Cette impl√©mentation via la Console GCP offre une interface graphique intuitive pour d√©velopper, tester et orchestrer votre pipeline de donn√©es BigQuery avec Dataform.