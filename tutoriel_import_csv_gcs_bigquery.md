# Tutoriel : Importer un fichier CSV depuis Google Cloud Storage vers BigQuery

## R√©f√©rence des bonnes pratiques
üìö [Bonnes pratiques pour charger, transformer et exporter des donn√©es BigQuery](https://cloud.google.com/bigquery/docs/load-transform-export-intro?hl=fr)

## Contexte
- **Public cible** : Data Engineers, Data Analysts
- **Environnement** : GCP Console (environnement DEV)
- **Stack** : BigQuery
- **Objectif** : Cr√©er un pipeline de donn√©es automatis√©

## Param√®tres du projet
- **Projet GCP** : `LakeHouse`
- **BigQuery name** : `lake-471013`
- **Dataset BigQuery** : `lakehouse_employee_data`
- **Bucket GCS** : `lakehouse-bucket-20250903`
- **Fichier CSV** : `employees_5mb.csv`
- **Chemin complet** : `gs://lakehouse-bucket-20250903/employees_5mb.csv`

## Pr√©requis
- Permissions appropri√©es sur BigQuery et GCS

## Plan du tutoriel

1. **[√âtape 1](#√©tape-1--v√©rification-du-fichier-csv-dans-gcs)** : V√©rification du fichier CSV dans GCS
2. **[√âtape 2](#√©tape-2--d√©veloppement-sur-console-gcp)** : D√©veloppement sur Console GCP
   - Cr√©ation du dataset BigQuery
   - D√©finition de la table et du sch√©ma
   - D√©veloppement du flux d'ingestion en SQL
3. **[√âtape 3](#√©tape-3--test-et-validation-du-flux)** : Test et validation du flux d'ingestion
4. **[√âtape 4](#√©tape-4--int√©gration-airflow-comme-orchestrateur)** : Int√©gration Airflow comme orchestrateur
   - Architecture du trigger Airflow
   - Configuration des connexions GCP
   - Monitoring et alertes
5. **[√âtape 5](#√©tape-5--d√©ploiement-et-monitoring-de-production)** : D√©ploiement et monitoring de production
6. **[√âtape 6](#√©tape-6--optimisations-pour-lentreprise)** : Optimisations pour l'entreprise
   - Partitioning et clustering
   - Gestion des donn√©es sensibles
7. **[Bonnes pratiques](#bonnes-pratiques)** et **[D√©pannage](#d√©pannage)**

## Structure du fichier CSV exemple
Le fichier contient les colonnes suivantes avec s√©parateur `;` :
```
id;nom;prenom;email;age;ville;code_postal;telephone;salaire;departement;date_embauche;statut;score;latitude;longitude;commentaire;reference;niveau;categorie;timestamp
```

## √âtape 1 : Pr√©paration et v√©rification du fichier CSV dans GCS

### 1.1 Cr√©ation du bucket GCS

1. Acc√©dez √† la **GCP Console**
2. Naviguez vers **Cloud Storage**
3. Cliquez sur **Cr√©er un bucket**
4. Configurez le bucket :
   - **Nom du bucket** : `lakehouse-bucket-20250903`
   - **Type d'emplacement** : R√©gion
   - **R√©gion** : `us-east1` (us pour le Free Tier)
   - **Classe de stockage** : Par d√©faut (Standard)
   - **Contr√¥le d'acc√®s** : Par d√©faut
   - **Protection** : Pas de protection
5. Cliquez sur **Cr√©er**

### 1.2 Upload du fichier CSV

1. S√©lectionnez le bucket `lakehouse-bucket-20250903` cr√©√©
2. Cliquez sur **Importer des fichiers**
3. S√©lectionnez votre fichier `employees_5mb.csv` depuis votre syst√®me local
4. Attendez la fin de l'upload
5. V√©rifiez que le fichier appara√Æt dans la liste avec la taille attendue (~5MB)

### 1.3 V√©rification du fichier upload√©

1. **V√©rifier le fichier upload√©** :
   - Cliquez sur le fichier `employees_5mb.csv` pour voir ses d√©tails
   - Notez le chemin complet : `gs://lakehouse-bucket-20250903/employees_5mb.csv`
   - V√©rifiez que la taille est d'environ 5MB

## √âtape 2 : D√©veloppement sur Console GCP

### Use Case : d√©veloppement du flux d'ingestion dans la console BigQuery

### 2.1 Cr√©ation du dataset BigQuery

1. Dans la **GCP Console**, acc√©dez √† **BigQuery**
2. Dans l'explorateur, cliquez sur votre projet `lake-471013`
3. Cliquez sur **Cr√©er un dataset**
4. Configurez le dataset :
   - **ID du dataset** : `lakehouse_employee_data`
   - **Emplacement** : US (pour correspondre au bucket GCS)
   - **Expiration** : Par d√©faut ou selon votre politique d'entreprise
5. Cliquez sur **Cr√©er un dataset**

### 2.1.1 Test d'accessibilit√© du fichier GCS

Une fois le dataset cr√©√©, v√©rifiez l'accessibilit√© du fichier CSV :

1. **Test d'accessibilit√© depuis BigQuery** :
   - Dans l'explorateur BigQuery, cliquez sur le dataset `lakehouse_employee_data` 
   - Cliquez sur **+ Cr√©er une table**
   - **Source** : Google Cloud Storage
   - **Parcourir** : chercher le fichier dans le bucket `lakehouse-bucket-20250903`
   
2. **Validation des permissions** :
   - Si vous pouvez parcourir et s√©lectionner le fichier ‚Üí Permissions OK
   - Si le fichier n'appara√Æt pas ‚Üí Contactez votre administrateur GCP

2. **Quitter sans sauvegarder** :
   - "Annuler" tout en bas puis "Oui quitter"

### 2.2 Cr√©ation de la table avec sch√©ma d√©fini

1. **Ouvrir l'√©diteur SQL** :
   - Dans BigQuery, cliquez sur **+** (en haut du canvas)
   - Une nouvelle fen√™tre d'√©diteur SQL s'ouvre

2. **Saisir la requ√™te de cr√©ation de table** :

```sql
-- Cr√©ation de la table employees avec sch√©ma typ√©
CREATE TABLE `lake-471013.lakehouse_employee_data.employees` (
  id INT64 NOT NULL,
  nom STRING,
  prenom STRING,
  email STRING,
  age INT64,
  ville STRING,
  code_postal STRING,
  telephone STRING,
  salaire FLOAT64,
  departement STRING,
  date_embauche DATE,
  statut STRING,
  score FLOAT64,
  latitude FLOAT64,
  longitude FLOAT64,
  commentaire STRING,
  reference STRING,
  niveau STRING,
  categorie STRING,
  timestamp TIMESTAMP,
  -- M√©tadonn√©es d'ingestion
  ingestion_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
  source_file STRING
);
```

3. **Ex√©cuter la requ√™te** :
   - Cliquez sur **Ex√©cuter** (bouton bleu) ou utilisez Ctrl+Enter
   - V√©rifiez que la table appara√Æt dans l'explorateur sous `lakehouse_employee_data`
   - La table est maintenant cr√©√©e et pr√™te pour l'ingestion

### 2.3 D√©veloppement du flux d'ingestion en SQL

#### A. Requ√™te de chargement depuis GCS

```sql
-- Flux d'ingestion principal depuis GCS avec sch√©ma forc√©
LOAD DATA INTO `lake-471013.lakehouse_employee_data.employees`
(id INT64, nom STRING, prenom STRING, email STRING, age INT64, ville STRING, 
 code_postal STRING, telephone STRING, salaire FLOAT64, departement STRING, 
 date_embauche DATE, statut STRING, score FLOAT64, latitude FLOAT64, 
 longitude FLOAT64, commentaire STRING, reference STRING, niveau STRING, 
 categorie STRING, timestamp TIMESTAMP)
FROM FILES (
  format = 'CSV',
  field_delimiter = ';',
  skip_leading_rows = 1,
  uris = ['gs://lakehouse-bucket-20250903/employees_5mb.csv']
);
```

**Avantages du sch√©ma forc√©** :
- **√âvite les conflits** : BigQuery n'essaie pas de d√©tecter automatiquement le sch√©ma
- **Contr√¥le total** : Impose exactement les types de donn√©es souhait√©s
- **Robustesse** : √âvite les erreurs "Field has changed mode/type"
- **Performance** : Pas de phase de d√©tection automatique

#### B. Requ√™te de validation et nettoyage

```sql
-- Validation et nettoyage des donn√©es
CREATE OR REPLACE TABLE `lake-471013.lakehouse_employee_data.employees_clean` AS
SELECT 
  id,
  TRIM(nom) as nom,
  TRIM(prenom) as prenom,
  LOWER(email) as email,
  CASE 
    WHEN age BETWEEN 18 AND 65 THEN age 
    ELSE NULL 
  END as age,
  ville,
  code_postal,
  telephone,
  salaire,
  departement,
  date_embauche,
  statut,
  score,
  latitude,
  longitude,
  commentaire,
  reference,
  niveau,
  categorie,
  timestamp,
  CURRENT_TIMESTAMP() as processed_date
FROM `lake-471013.lakehouse_employee_data.employees`
WHERE id IS NOT NULL;
```

#### C. Contr√¥les qualit√© int√©gr√©s

```sql
-- Requ√™tes de contr√¥le qualit√©
SELECT 
  'Total lignes' as metric,
  COUNT(*) as valeur
FROM `lake-471013.lakehouse_employee_data.employees_clean`

UNION ALL

SELECT 
  'Emails invalides' as metric,
  COUNT(*) as valeur  
FROM `lake-471013.lakehouse_employee_data.employees_clean`
WHERE email NOT LIKE '%@%'

UNION ALL

SELECT 
  'Donn√©es manquantes critiques' as metric,
  COUNT(*) as valeur
FROM `lake-471013.lakehouse_employee_data.employees_clean`
WHERE nom IS NULL OR prenom IS NULL;
```

## √âtape 3 : Test et validation du flux

### 3.1 Tests unitaires des transformations

Le Data Engineer teste chaque √©tape du flux :

1. **Test de chargement** : V√©rification que les donn√©es sont correctement import√©es depuis GCS
2. **Test de transformation** : Validation des r√®gles de nettoyage et normalisation
3. **Test de qualit√©** : Contr√¥le des m√©triques de qualit√© des donn√©es

### 3.2 Validation des performances

```sql
-- Analyse des performances d'ingestion
SELECT 
  job_id,
  creation_time,
  start_time,
  end_time,
  total_bytes_processed,
  total_slot_ms
FROM `region-europe-west1.INFORMATION_SCHEMA.JOBS_BY_PROJECT`
WHERE job_type = 'LOAD'
AND creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY);
```

### 3.3 Tests de r√©gression

Le Data Engineer s'assure que les modifications n'impactent pas les donn√©es existantes :
- Comparaison des m√©triques avant/apr√®s
- Validation des sch√©mas
- Tests de non-r√©gression sur les requ√™tes aval

## √âtape 4 : Int√©gration Airflow comme orchestrateur

### R√¥le d'Airflow : Trigger du flux d'ingestion

Une fois que le Data Engineer a d√©velopp√© et test√© le flux d'ingestion en SQL sur la **Console GCP**, **Apache Airflow** intervient uniquement comme **orchestrateur et trigger** du processus. Airflow n'ex√©cute pas directement le code SQL mais d√©clenche l'ex√©cution des requ√™tes d√©velopp√©es.

### 4.1 Architecture du trigger Airflow

**Principe** : Airflow orchestre l'ex√©cution s√©quentielle des requ√™tes SQL d√©velopp√©es par le Data Engineer.

#### Flux d'orchestration :

1. **D√©tection de fichier** : Airflow surveille l'arriv√©e de nouveaux fichiers CSV dans GCS
2. **Trigger d'ingestion** : Lancement du flux d'ingestion BigQuery d√©velopp√©
3. **Monitoring d'ex√©cution** : Suivi de l'√©tat des jobs BigQuery
4. **Validation qualit√©** : Ex√©cution des contr√¥les qualit√©
5. **Notifications** : Alertes en cas de succ√®s/√©chec

### 4.2 Int√©gration avec les d√©veloppements GCP

#### A. Ex√©cution des requ√™tes SQL d√©velopp√©es

Airflow utilise les **operators BigQuery** pour d√©clencher les requ√™tes SQL cr√©√©es par le Data Engineer :

- **BigQueryInsertJobOperator** : Ex√©cute les requ√™tes `LOAD DATA` d√©velopp√©es
- **BigQueryCreateEmptyTableOperator** : Cr√©e les tables si n√©cessaire
- **BigQueryCheckOperator** : Lance les contr√¥les qualit√© SQL

#### B. Gestion des param√®tres dynamiques

Airflow injecte des param√®tres dynamiques dans les requ√™tes SQL :

- **Date d'ex√©cution** : `{{ ds }}` pour les partitions temporelles
- **Fichiers source** : D√©tection automatique des nouveaux fichiers CSV
- **Variables d'environnement** : Diff√©renciation dev/staging/prod

#### C. Orchestration des √©tapes

L'orchestrateur Airflow s√©quence l'ex√©cution :

1. **Pr√©-validation** ‚Üí V√©rification fichier source
2. **Ingestion** ‚Üí Ex√©cution du `LOAD DATA` d√©velopp√©
3. **Transformation** ‚Üí Application des r√®gles de nettoyage SQL
4. **Contr√¥le qualit√©** ‚Üí Ex√©cution des requ√™tes de validation
5. **Post-traitement** ‚Üí Notifications et m√©triques

### 4.3 Configuration de l'int√©gration

#### Connexions GCP dans Airflow

L'int√©gration n√©cessite la configuration des connexions :

- **Service Account** : Cl√© de service avec permissions BigQuery et GCS
- **Project ID** : lake-471013
- **Default location** : R√©gion pour l'ex√©cution des jobs

#### Variables d'environnement

Airflow g√®re les param√®tres via des variables :

- **Dataset names** : `lakehouse_employee_data`, `lakehouse_employee_data_staging`
- **GCS paths** : `gs://lakehouse-bucket-20250903/`
- **Notification emails** : Destinataires des alertes
- **Retry policies** : Nombre de tentatives et d√©lais

### 4.4 Monitoring et alertes via Airflow

#### Tableaux de bord Airflow

1. **DAG View** : Visualisation du flux d'orchestration
2. **Task logs** : Logs d√©taill√©s de chaque √©tape BigQuery
3. **Gantt Chart** : Analyse des performances temporelles

#### Int√©gration monitoring GCP

Airflow remonte les m√©triques vers les outils de monitoring GCP :

- **Cloud Logging** : Centralisation des logs d'ex√©cution
- **Cloud Monitoring** : M√©triques de performance des jobs
- **Cloud Alerting** : Notifications automatis√©es

#### Alertes configur√©es

1. **√âchec d'ingestion** : Notification imm√©diate aux Data Engineers
2. **D√©passement SLA** : Alerte si le traitement d√©passe le temps allou√©
3. **Anomalies qualit√©** : Notification si les contr√¥les d√©tectent des probl√®mes
4. **Ressources** : Alerte si consommation excessive de slots BigQuery

### 4.5 Gestion de l'environnement d'entreprise

#### S√©paration des environnements

Airflow orchestre diff√©rents environnements :

- **DEV** : Tests et d√©veloppements sur datasets de d√©veloppement
- **STAGING** : Validation avec donn√©es anonymis√©es
- **PRODUCTION** : Ex√©cution sur les donn√©es r√©elles

#### Gestion des permissions

L'orchestrateur Airflow respecte la gouvernance des donn√©es :

- **R√¥les IAM** : S√©paration des acc√®s par environnement
- **Service Accounts** : Comptes d√©di√©s par type d'op√©ration
- **Audit Trail** : Tra√ßabilit√© compl√®te des ex√©cutions

#### Strat√©gies de d√©ploiement

1. **Cloud Composer** : Service manag√© GCP recommand√© pour l'entreprise
2. **Airflow sur GKE** : D√©ploiement containeris√© avec contr√¥le total
3. **VM instances** : Solution simple pour environnements de d√©veloppement

### 4.6 Avantages de cette approche

**S√©paration des responsabilit√©s** :
- **Data Engineers** : Focus sur la logique m√©tier et les transformations SQL
- **Airflow** : Orchestration, scheduling et monitoring
- **GCP BigQuery** : Ex√©cution performante des requ√™tes

**Flexibilit√©** :
- Modifications SQL sans red√©ploiement Airflow
- Tests ind√©pendants des requ√™tes sur Console GCP
- R√©utilisabilit√© des flux pour diff√©rents datasets

**Monitoring centralis√©** :
- Vision unifi√©e de tous les pipelines de donn√©es
- Alertes coh√©rentes pour tous les flux d'ingestion
- M√©triques de performance agr√©g√©es

## √âtape 5 : D√©ploiement et monitoring de production

### V√©rification des donn√©es

```sql
-- V√©rifier l'import
SELECT COUNT(*) as total_rows
FROM `lake-471013.lakehouse_employee_data.employees`;

-- Aper√ßu des donn√©es
SELECT *
FROM `lake-471013.lakehouse_employee_data.employees`
LIMIT 10;
```

### Monitoring

1. **Cloud Logging** : Surveillez les logs d'import
2. **BigQuery Jobs** : V√©rifiez l'historique des jobs d'import
3. **Cloud Monitoring** : Cr√©ez des alertes sur les √©checs d'import

## √âtape 6 : Optimisations pour l'entreprise

### Partitioning et Clustering

```sql
-- Cr√©er une table partitionn√©e par date d'embauche
CREATE TABLE `lake-471013.lakehouse_employee_data.employees_partitioned`
PARTITION BY DATE(date_embauche)
CLUSTER BY departement, ville
AS SELECT * FROM `lake-471013.lakehouse_employee_data.employees`;
```

### Gestion des donn√©es sensibles

1. **Data Loss Prevention (DLP)** : Scanner les donn√©es sensibles
2. **IAM** : Contr√¥ler l'acc√®s aux donn√©es
3. **Audit Logs** : Tracer les acc√®s aux donn√©es

## Bonnes pratiques

- **Validation des donn√©es** : Impl√©mentez des contr√¥les qualit√©
- **Sauvegarde** : Conservez les versions pr√©c√©dentes
- **Documentation** : Maintenez la documentation √† jour
- **Tests** : Testez les imports en environnement de d√©veloppement
- **Monitoring** : Surveillez les performances et co√ªts

## D√©pannage

### Erreurs communes

- **Erreur de sch√©ma** : V√©rifiez le format des donn√©es
- **Permissions insuffisantes** : Validez les r√¥les IAM
- **Quota d√©pass√©** : Surveillez les limites BigQuery
- **Format CSV incorrect** : V√©rifiez les s√©parateurs et encodage

### Logs utiles

```bash
# Logs Cloud Function
gcloud logs read "resource.type=cloud_function" --limit 50

# Jobs BigQuery
bq ls -j --max_results 10
```

Ce pipeline automatis√© permettra l'import r√©gulier de vos fichiers CSV depuis GCS vers BigQuery, avec monitoring et gestion d'erreurs appropri√©s pour un environnement d'entreprise.